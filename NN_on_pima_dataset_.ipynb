{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUZUJInm1j6OQ8o3utwnYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanilAD/Corporate-Training-Mini_Projects/blob/main/NN_on_pima_dataset_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this mini-project, I implemented a neural network model for binary classification using Keras and Scikit-learn. I started by preprocessing the dataset, splitting it into training and testing sets, and scaling the features. I then used GridSearchCV to tune hyperparameters such as learning rate and dropout rate. After finding the best hyperparameters, I created a final model and trained it on the training set. Finally, I evaluated the model on both the training and testing sets to measure its performance. This project helped me understand the importance of hyperparameter tuning and how it can improve the performance of a neural network model."
      ],
      "metadata": {
        "id": "j7yh_hjUG34O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/diabetes.csv\")"
      ],
      "metadata": {
        "id": "e-yG5InGwsc1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into input (X) and output (Y) variables\n",
        "X = df.drop('Outcome', axis=1)\n",
        "Y = df['Outcome']"
      ],
      "metadata": {
        "id": "9Lj5y0DjoX44"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "7cW_cdAMoaum"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "TO03x2PKoflp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base model\n",
        "def create_model(optimizer='adam', activation='relu', kernel_initializer='glorot_uniform', neurons=1, dropout_rate=0.0):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, input_dim=8, kernel_initializer=kernel_initializer, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, kernel_initializer=kernel_initializer, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "SqnODhB1oj1o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create classifier model\n",
        "classifier = KerasClassifier(build_fn=create_model, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UdgtqDYhonlL",
        "outputId": "5dc0fea5-cd6d-4702-eb9b-38a191c9eda1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-13fe41dfede8>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  classifier = KerasClassifier(build_fn=create_model, verbose=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# Perform grid search for batch size and epochs\n",
        "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Summarize results for batch size and epochs\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TlWMQ8Amoqnm",
        "outputId": "c478226d-2de2-4bea-c748-3f9117067231"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.765471 using {'batch_size': 20, 'epochs': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Define the grid search parameters\n",
        "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "param_grid = dict(learning_rate=learning_rate)\n",
        "\n",
        "# Create a function to build the model with the specified learning rate\n",
        "def create_model(learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)\n",
        "\n",
        "# Perform grid search for learning rate\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Summarize results for learning rate\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iC4n_x3gotoR",
        "outputId": "aba89138-71b3-405d-f531-c609dcba2df5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-808df8d1303c>:17: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.747553 using {'learning_rate': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid search parameters\n",
        "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "kernel_initializer = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "param_grid = dict(activation=activation, kernel_initializer=kernel_initializer)\n",
        "\n",
        "# Perform grid search for activation function and kernel initializer\n",
        "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Summarize results for activation function and kernel initializer\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4P8fqyNSo_fo",
        "outputId": "a6e6593c-baee-4d6d-d500-b2e1c8365373"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.656377 using {'activation': 'softsign', 'kernel_initializer': 'normal'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid search parameters\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(neurons=neurons)\n",
        "\n",
        "# Perform grid search for number of neurons in activation layer\n",
        "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Summarize results for number of neurons in activation layer\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eM0DfnhEpEaS",
        "outputId": "0c273432-4e98-4d51-b9ca-8dc4a2a32f92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.617161 using {'neurons': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(optimizer='adam', activation='relu', kernel_initializer='glorot_uniform', neurons=1, dropout_rate=0.0):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons, input_dim=X_train_scaled.shape[1], kernel_initializer=kernel_initializer, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the final model with optimum hyperparameters\n",
        "best_params = grid_result.best_params_\n",
        "\n",
        "# Extract the best hyperparameters or use default values if not found\n",
        "optimizer = best_params.get('optimizer', 'adam')\n",
        "activation = best_params.get('activation', 'relu')\n",
        "kernel_initializer = best_params.get('kernel_initializer', 'glorot_uniform')\n",
        "neurons = best_params.get('neurons', 1)\n",
        "dropout_rate = best_params.get('dropout_rate', 0.0)\n",
        "\n",
        "final_model = create_model(optimizer=optimizer, activation=activation,\n",
        "                            kernel_initializer=kernel_initializer, neurons=neurons,\n",
        "                            dropout_rate=dropout_rate)\n",
        "\n",
        "# Train the final model with optimum hyperparameters\n",
        "final_model.fit(X_train_scaled, Y_train, epochs=25, batch_size=10)\n",
        "\n",
        "# Evaluate the final model\n",
        "Y_pred = final_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(Y_test, Y_pred.round())\n",
        "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100))\n",
        "\n",
        "# Evaluate the final model on training data\n",
        "train_loss, train_accuracy = final_model.evaluate(X_train_scaled, Y_train, verbose=0)\n",
        "print(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4FXZeQZvpIbm",
        "outputId": "68286086-f14f-4f10-98db-1f0ac767cfd5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/28\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 0.5580 - accuracy: 0.7443\n",
            "Epoch 2/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7638\n",
            "Epoch 3/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7655\n",
            "Epoch 4/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7720\n",
            "Epoch 5/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7785\n",
            "Epoch 6/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7834\n",
            "Epoch 7/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7801\n",
            "Epoch 8/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7834\n",
            "Epoch 9/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7915\n",
            "Epoch 10/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4531 - accuracy: 0.7866\n",
            "Epoch 11/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.7899\n",
            "Epoch 12/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.7899\n",
            "Epoch 13/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.7932\n",
            "Epoch 14/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.7948\n",
            "Epoch 15/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7980\n",
            "Epoch 16/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7932\n",
            "Epoch 17/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7997\n",
            "Epoch 18/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.7980\n",
            "Epoch 19/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7948\n",
            "Epoch 20/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7932\n",
            "Epoch 21/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.7980\n",
            "Epoch 22/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7980\n",
            "Epoch 23/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.7980\n",
            "Epoch 24/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7997\n",
            "Epoch 25/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7964\n",
            "Epoch 26/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.7948\n",
            "Epoch 27/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7948\n",
            "Epoch 28/28\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.7980\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "Test Accuracy: 80.52%\n",
            "Train Accuracy: 79.64%\n"
          ]
        }
      ]
    }
  ]
}